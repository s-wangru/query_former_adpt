{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries_from_file(file_path):\n",
    "    queries = []\n",
    "    current_query = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('--') or not line:\n",
    "                continue\n",
    "            \n",
    "            current_query.append(line)\n",
    "\n",
    "            if line.endswith(';'):\n",
    "                queries.append(' '.join(current_query))\n",
    "                current_query = []  # Reset for the next query\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "queries = read_queries_from_file('tpch-kit/dbgen/tpch-stream.sql')\n",
    "print(f\"Total Queries Read: {len(queries)}\")\n",
    "print(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_query_plan(query):\n",
    "    \"\"\"Function to get the query execution plan for a given SQL query\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            database=\"tpch\",\n",
    "            user=\"ruiqiwan\",\n",
    "            password=\"admin\",\n",
    "            host=\"127.0.0.1\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"EXPLAIN (ANALYZE, FORMAT JSON) {query}\")\n",
    "        plan = cur.fetchone()[0]  # fetchone() returns a tuple\n",
    "        return plan[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting plan for query: {query}\\n{e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "query_plans = []\n",
    "for i, query in enumerate(queries):\n",
    "    plan = get_query_plan(query)\n",
    "    if plan:\n",
    "        query_plans.append({\"id\": i, \"json\": json.dumps(plan)})\n",
    "        # query_plans.append({\"id\": i, \"json\": json.dumps(plan), \"sql\": query})\n",
    "\n",
    "\n",
    "full_train_df = pd.DataFrame(query_plans)\n",
    "\n",
    "full_train_df.to_csv('query_plans.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_train_df.shape)\n",
    "\n",
    "print(full_train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example of loading JSON to ensure it's correctly formatted\n",
    "for i, row in full_train_df.iterrows():\n",
    "    try:\n",
    "        json_obj = json.loads(row['json'])\n",
    "        # Optionally, re-serialize the object for standard formatting\n",
    "        full_train_df.at[i, 'json'] = json.dumps(json_obj)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON for id {row['id']}: {e}\")\n",
    "        # Optionally, drop the row if the JSON is not valid\n",
    "        full_train_df.drop(i, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_alias = {'nation' : 'n',\n",
    "            'region' : 'r',\n",
    "            'supplier' : 's',\n",
    "            'part' : 'p',\n",
    "            'partsupp' : 'ps',\n",
    "            'customer' : 'c',\n",
    "            'orders' : 'o',\n",
    "            'lineitem' : 'l'}\n",
    "\n",
    "alias_to_db = {v: k for k, v in db_alias.items()}\n",
    "\n",
    "column_mapping = {\n",
    "    \"nation\": {\n",
    "        0: \"n_nationkey\",\n",
    "        1: \"n_name\",\n",
    "        2: \"n_regionkey\",\n",
    "        3: \"n_comment\"\n",
    "    },\n",
    "    \"region\": {\n",
    "        0: \"r_regionkey\",\n",
    "        1: \"r_name\",\n",
    "        2: \"r_comment\"\n",
    "    },\n",
    "    \"part\": {\n",
    "        0: \"p_partkey\",\n",
    "        1: \"p_name\",\n",
    "        2: \"p_mfgr\",\n",
    "        3: \"p_brand\",\n",
    "        4: \"p_type\",\n",
    "        5: \"p_size\",\n",
    "        6: \"p_container\",\n",
    "        7: \"p_retailprice\",\n",
    "        8: \"p_comment\"\n",
    "    },\n",
    "    \"supplier\": {\n",
    "        0: \"s_suppkey\",\n",
    "        1: \"s_name\",\n",
    "        2: \"s_address\",\n",
    "        3: \"s_nationkey\",\n",
    "        4: \"s_phone\",\n",
    "        5: \"s_acctbal\",\n",
    "        6: \"s_comment\"\n",
    "    },\n",
    "    \"partsupp\": {\n",
    "        0: \"ps_partkey\",\n",
    "        1: \"ps_suppkey\",\n",
    "        2: \"ps_availqty\",\n",
    "        3: \"ps_supplycost\",\n",
    "        4: \"ps_comment\"\n",
    "    },\n",
    "    \"customer\": {\n",
    "        0: \"c_custkey\",\n",
    "        1: \"c_name\",\n",
    "        2: \"c_address\",\n",
    "        3: \"c_nationkey\",\n",
    "        4: \"c_phone\",\n",
    "        5: \"c_acctbal\",\n",
    "        6: \"c_mktsegment\",\n",
    "        7: \"c_comment\"\n",
    "    },\n",
    "    \"orders\": {\n",
    "        0: \"o_orderkey\",\n",
    "        1: \"o_custkey\",\n",
    "        2: \"o_orderstatus\",\n",
    "        3: \"o_totalprice\",\n",
    "        4: \"o_orderdate\",\n",
    "        5: \"o_orderpriority\",\n",
    "        6: \"o_clerk\",\n",
    "        7: \"o_shippriority\",\n",
    "        8: \"o_comment\"\n",
    "    },\n",
    "    \"lineitem\": {\n",
    "        0: \"l_orderkey\",\n",
    "        1: \"l_partkey\",\n",
    "        2: \"l_suppkey\",\n",
    "        3: \"l_linenumber\",\n",
    "        4: \"l_quantity\",\n",
    "        5: \"l_extendedprice\",\n",
    "        6: \"l_discount\",\n",
    "        7: \"l_tax\",\n",
    "        8: \"l_returnflag\",\n",
    "        9: \"l_linestatus\",\n",
    "        10: \"l_shipdate\",\n",
    "        11: \"l_commitdate\",\n",
    "        12: \"l_receiptdate\",\n",
    "        13: \"l_shipinstruct\",\n",
    "        14: \"l_shipmode\",\n",
    "        15: \"l_comment\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "column_min_max_vals = {}\n",
    "col2idx = {}\n",
    "current_index = 0\n",
    "\n",
    "# def convert_to_numeric(val):\n",
    "#     if isinstance(val, str):\n",
    "#         # Convert string to numeric using a hash-based encoding (you could use other schemes too)\n",
    "#         numeric_value = int.from_bytes(val.encode('utf-8'), 'big') % int(1e9)\n",
    "#     elif pd.isnull(val):\n",
    "#         # Handle NaN values\n",
    "#         numeric_value = 0\n",
    "#     else:\n",
    "#         numeric_value = val\n",
    "#     return numeric_value\n",
    "\n",
    "def extract_minmax(file_path):\n",
    "    global current_index\n",
    "    \n",
    "    df = pd.read_table('tpch-kit/dbgen/outputs/' + file_path + \".tbl\", delimiter='|', header=None, index_col=False)\n",
    "\n",
    "    alias = db_alias[file_path]\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_name = column_mapping[file_path][col]\n",
    "        alias = db_alias[file_path]\n",
    "\n",
    "        # df[col] = df[col].apply(convert_to_numeric)\n",
    "\n",
    "        min = df[col].min()\n",
    "        max = df[col].max()\n",
    "        if isinstance(min, str) or isinstance(max, str):\n",
    "            min = int.from_bytes(min.encode('utf-8'), 'big')\n",
    "            max = int.from_bytes(max.encode('utf-8'), 'big')\n",
    "        column_min_max_vals[alias + '.' + col_name] = [min, max]\n",
    "\n",
    "        if col_name not in col2idx:\n",
    "            col2idx[alias + '.' + col_name] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "    col2idx['NA'] = current_index\n",
    "    return column_min_max_vals\n",
    "\n",
    "extract_minmax('customer')\n",
    "extract_minmax('lineitem')\n",
    "extract_minmax('nation')\n",
    "extract_minmax('orders')\n",
    "extract_minmax('part')\n",
    "extract_minmax('partsupp')\n",
    "extract_minmax('region')\n",
    "extract_minmax('supplier')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.database_util import Encoding\n",
    "\n",
    "encoding = Encoding(column_min_max_vals, col2idx, op2idx = {'=': 0, '>': 1, '<': 2, '>=': 3, '<=': 4, 'NA' : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for json_string in full_train_df['json']:\n",
    "    query_plan = json.loads(json_string)['Plan']\n",
    "    def traverse_plan(plan):\n",
    "        encoding.encode_type(plan['Node Type'])\n",
    "        if 'Plans' in plan:\n",
    "            for subplan in plan['Plans']:\n",
    "                traverse_plan(subplan)\n",
    "\n",
    "    traverse_plan(query_plan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(encoding.col2idx)\n",
    "print(encoding.idx2col)\n",
    "print(encoding.column_min_max_vals)\n",
    "print(encoding.idx2type)\n",
    "print(encoding.type2idx)\n",
    "\n",
    "encoding.idx2col[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.save({'encoding': encoding}, 'checkpoints/tpch_encoding.pt')\n",
    "\n",
    "encoding_ckpt = torch.load('checkpoints/tpch_encoding.pt')\n",
    "encoding = encoding_ckpt['encoding']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "        database=\"tpch\",\n",
    "        user=\"ruiqiwang\",\n",
    "        password=\"admin\",\n",
    "        host=\"127.0.0.1\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "hist_file = pd.DataFrame(columns=['table', 'column', 'bins', 'table_column', 'freq'])\n",
    "\n",
    "def to_vals(data_list):\n",
    "    for dat in data_list:\n",
    "        val = dat[0]\n",
    "        if val is not None: break\n",
    "    try:\n",
    "        float(val)\n",
    "        return np.array(data_list, dtype=float).squeeze()\n",
    "    except (ValueError, TypeError):\n",
    "        res = []\n",
    "        for dat in data_list:\n",
    "            val = dat[0]\n",
    "            if val is None:\n",
    "                res.append(0)  \n",
    "            elif isinstance(val, str):\n",
    "                hex_value = int.from_bytes(val.encode('utf-8'), 'big')\n",
    "                normalized_hex = hex_value % int(1e9)  \n",
    "                res.append(normalized_hex)\n",
    "            else:\n",
    "                try:\n",
    "                    mi = val.timestamp()\n",
    "                except AttributeError:\n",
    "                    mi = 0\n",
    "                res.append(mi)\n",
    "        \n",
    "        return np.array(res)\n",
    "\n",
    "for table, alias in db_alias.items():\n",
    "    for column in column_mapping[table].values():\n",
    "        cmd = f'SELECT {column} FROM {table}'\n",
    "        cur.execute(cmd)\n",
    "        col = cur.fetchall()\n",
    "        col_array = to_vals(col)\n",
    "        \n",
    "        hists = np.nanpercentile(col_array, range(0, 101, 2), axis=0)\n",
    "        \n",
    "        freq = np.histogram(col_array, bins=hists)[0]\n",
    "        \n",
    "        freq_hex = freq.astype('float').tobytes().hex()\n",
    "        \n",
    "        # Store results in the dataframe\n",
    "        res_dict = {\n",
    "            'table': table,\n",
    "            'column': column,\n",
    "            'bins': repr((' ').join([str(int(i)) for i in hists])),  \n",
    "            'freq': freq_hex,\n",
    "            'table_column': f'{alias}.{column}' \n",
    "        }\n",
    "        hist_file = pd.concat([hist_file, pd.DataFrame([res_dict])], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_file\n",
    "hist_file.to_csv('histograms.csv', index=False)\n",
    "\n",
    "hist_file = pd.read_csv('histograms.csv')\n",
    "hist_file.head()\n",
    "len(hist_file['bins'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = 'CREATE EXTENSION tsm_system_rows'\n",
    "cur.execute(cmd)\n",
    "\n",
    "tables = list(db_alias.keys())\n",
    "sample_data = {}\n",
    "for table in tables:\n",
    "    cur.execute(\"Select * FROM {} LIMIT 0\".format(table))\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "    ts = pd.DataFrame(columns = colnames)\n",
    "\n",
    "    for num in range(1000):\n",
    "        cmd = 'SELECT * FROM {} TABLESAMPLE SYSTEM_ROWS(1)'.format(table)\n",
    "        cur.execute(cmd)\n",
    "        samples = cur.fetchall()\n",
    "        for i,row in enumerate(samples):\n",
    "            ts.loc[num]=row\n",
    "    \n",
    "    sample_data[table] = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "engine = create_engine('postgresql://ruiqiwang:admin@localhost:5432/tpch_sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sample_data.items():\n",
    "    v['sid'] = list(range(1000))\n",
    "    try:\n",
    "        cmd = 'ALTER TABLE {} ADD COLUMN IF NOT EXISTS sid INTEGER'.format(k)\n",
    "        cur.execute(cmd)\n",
    "        v.to_sql(k,engine,if_exists='append',index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predicate(predicate):\n",
    "        parsed_conditions = []\n",
    "\n",
    "        conditions = predicate.split(' AND ')\n",
    "        for condition in conditions:\n",
    "            condition = condition.replace('(', '').replace(')', '')\n",
    "            print(condition)\n",
    "            # Extract column, operator, value\n",
    "            for operator in ['>=', '<=', '<>', '=', '>', '<', 'LIKE']:\n",
    "                if operator in condition:\n",
    "                    parts = condition.split(operator)\n",
    "                    if len(parts) == 2:\n",
    "                        column, value = parts\n",
    "                        column = column.strip()\n",
    "                        value = value.strip()\n",
    "                        prefixed_column = column\n",
    "                        if len(column.split('_')) > 1:\n",
    "                            prefix = column.split('_')[0]\n",
    "                            prefixed_column = prefix + '.' + column\n",
    "                        parsed_conditions.append(f\"{prefixed_column},{operator},{value}\")\n",
    "                    break\n",
    "        return parsed_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def parse_query_plan(query_plan):\n",
    "    tables = set()\n",
    "    joins = []\n",
    "    predicates = []\n",
    "    cardinality = None\n",
    "\n",
    "    def traverse_plan(plan):\n",
    "        nonlocal cardinality\n",
    "        node_type = plan.get('Node Type', '')\n",
    "        \n",
    "        if node_type in ['Seq Scan', 'Index Scan']:\n",
    "            relation_name = plan.get('Relation Name', '')\n",
    "            alias = plan.get('Alias', '')\n",
    "            if relation_name and alias:\n",
    "                tables.add(f\"{relation_name} {alias}\")\n",
    "\n",
    "        if node_type in ['Nested Loop', 'Hash Join', 'Merge Join']:\n",
    "            join_condition = plan.get('Join Filter', plan.get('Hash Cond', ''))\n",
    "            if join_condition:\n",
    "                joins.append(join_condition)\n",
    "\n",
    "        if 'Filter' in plan:\n",
    "            filter_condition = plan['Filter']\n",
    "            parsed_predicates = parse_predicate(filter_condition)\n",
    "            predicates.extend(parsed_predicates)\n",
    "        \n",
    "        if 'Actual Rows' in plan:\n",
    "            cardinality = plan['Actual Rows']\n",
    "        elif 'Plan Rows' in plan and cardinality is None:\n",
    "            cardinality = plan['Plan Rows']\n",
    "        \n",
    "\n",
    "        if 'Plans' in plan:\n",
    "            for subplan in plan['Plans']:\n",
    "                traverse_plan(subplan)\n",
    "\n",
    "    traverse_plan(query_plan)\n",
    "\n",
    "    # Format the result\n",
    "    table_str = ','.join(tables)\n",
    "    join_str = ','.join(joins)\n",
    "    predicate_str = ','.join(predicates)\n",
    "    card_str = str(cardinality) if cardinality else 'N/A'\n",
    "\n",
    "    return f\"{table_str}#{join_str}#{predicate_str}#{card_str}\"\n",
    "\n",
    "parsed_results = []\n",
    "\n",
    "for query_plan in query_plans:\n",
    "    plan_dict = json.loads(query_plan['json'])\n",
    "    parsed_result = parse_query_plan(plan_dict['Plan'])\n",
    "    parsed_results.append(parsed_result)\n",
    "\n",
    "df = pd.DataFrame(parsed_results, columns=[\"parsed_plan\"])\n",
    "\n",
    "\n",
    "df.to_csv('tpch.csv', index=False, header=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "\n",
    "print(\"Parsed query plans have been saved to 'parsed_query_plans.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = pd.read_csv('tpch.csv',sep='#',header=None)\n",
    "query_file.columns = ['table','join','predicate','card']\n",
    "\n",
    "query_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conm = psycopg2.connect(database=\"tpch_sample\", user=\"ruiqiwang\", host=\"127.0.0.1\",password=\"admin\", port=\"5432\")\n",
    "conm.set_session(autocommit=True)\n",
    "cur = conm.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "table_samples = []\n",
    "for i,row in query_file.iterrows():\n",
    "    table_sample = {}\n",
    "    preds = row['predicate'].split(',')\n",
    "    for i in range(0,len(preds),3):\n",
    "        left, op, right = preds[i:i+3]\n",
    "        alias,col = left.strip().split('.')\n",
    "        pred_string = ''.join((col,op,right))\n",
    "        q = 'select sid from {} where {}'.format(alias_to_db[alias], pred_string)\n",
    "        cur.execute(q)\n",
    "        sps = np.zeros(1000).astype('uint8')\n",
    "        sids = cur.fetchall()\n",
    "        sids = np.array(sids).squeeze()\n",
    "        if sids.size>1:\n",
    "            sps[sids] = 1\n",
    "        if alias_to_db[alias] in table_sample:\n",
    "            table_sample[alias_to_db[alias]] = table_sample[alias_to_db[alias]] & sps\n",
    "        else:\n",
    "            table_sample[alias_to_db[alias]] = sps\n",
    "    table_samples.append(table_sample)\n",
    "    if pd.isnull(row['join']):\n",
    "        continue\n",
    "    joins = row['join'].split(',')\n",
    "    for join in joins:\n",
    "        alias1,alias2 = join.split('=')[0].strip().split(' ')[0],join.split('=')[1].strip().split(' ')[0]\n",
    "        alias1,alias2 = alias1.split('.')[0][1:],alias2.split('.')[0]\n",
    "        print(alias1,alias2)\n",
    "        table1,table2 = alias_to_db[alias1],alias_to_db[alias2]\n",
    "        q = 'select {}.sid from {} {} join {} {} on {}'.format(alias1, table1, alias1, table2, alias2,join)\n",
    "        cur.execute(q)\n",
    "        sids = cur.fetchall()\n",
    "        sids = np.array(sids).squeeze()\n",
    "        if sids.size>1:\n",
    "            sps[sids] = 1\n",
    "        if alias1 in table_sample:\n",
    "            table_sample[alias1] = table_sample[alias1] & sps\n",
    "        else:\n",
    "            table_sample[alias1] = sps\n",
    "        q = 'select {}.sid from {} {} join {} {} on {}'.format(alias2, table1, alias1, table2, alias2,join)\n",
    "        cur.execute(q)\n",
    "        sids = cur.fetchall()\n",
    "        sids = np.array(sids).squeeze()\n",
    "        if sids.size>1:\n",
    "            sps[sids] = 1\n",
    "        if alias2 in table_sample:\n",
    "            table_sample[alias2] = table_sample[alias2] & sps\n",
    "        else:\n",
    "            table_sample[alias2] = sps\n",
    "\n",
    "bitmap_csv_file = \"tpch.bitmaps\"\n",
    "with open(bitmap_csv_file, 'wb') as f:\n",
    "    for table_sample in table_samples:\n",
    "        # Write the number of tables for this query\n",
    "        num_tables = len(table_sample)\n",
    "        f.write(num_tables.to_bytes(4, byteorder='little'))\n",
    "        for alias, bitmap in table_sample.items():\n",
    "            # Pack the bitmap into bytes\n",
    "            num_bytes_per_bitmap = (len(bitmap) + 7) // 8\n",
    "            bitmap_bytes = np.packbits(bitmap[:num_bytes_per_bitmap * 8])\n",
    "            f.write(bitmap_bytes)\n",
    "\n",
    "print(f\"Bitmap CSV saved as {bitmap_csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cur.close()\n",
    "# conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
